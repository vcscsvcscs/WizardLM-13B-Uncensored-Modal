# Autogenerated from: app.py

# ## Basic setup
from __future__ import annotations

from typing import Optional

from fastapi import FastAPI, Header
from pydantic import BaseModel

from modal import Image, Stub, asgi_app, method, gpu

# All Modal programs need a [`Stub`](/docs/reference/modal.Stub) â€” an object that acts as a recipe for
# the application. Let's give it a friendly name.

stub = Stub("WizardLM-13B")

cache_path = "/vol/cache"
MODEL_ID = "ehartford/WizardLM-13B-Uncensored"
HF_TOKEN = ''
CHUNK_SIZE = 1024 * 1024
LOAD_8BIT = False

def download_models():
    from transformers import  LlamaTokenizer, LlamaForCausalLM

    # do a dry run of loading the huggingface model, which will download weights
    tokenizer =  LlamaTokenizer.from_pretrained(MODEL_ID)
    model = LlamaForCausalLM.from_pretrained(MODEL_ID)


# ## Model dependencies
#
# Your model will be running remotely inside a container. We will be installing
# all the model dependencies in the next step. We will also be "baking the model"
# into the image by running a Python function as a part of building the image.
# This lets us start containers much faster, since all the data that's needed is
# already inside the image.

image = (
    Image.debian_slim(python_version="3.10")
    .pip_install(
        "accelerate",
        "ftfy",
        "torchvision", 
        "torch>=2.0.0+cu117",
        "sentencepiece",
    ).apt_install("git")
    .pip_install("git+https://github.com/huggingface/transformers","torch>=2.0.0+cu117")
    .run_function(
        download_models,
    )
)
stub.image = image

# ## Using container lifecycle methods
#
# Modal lets you implement code that runs every time a container starts. This
# can be a huge optimization when you're calling a function multiple times,
# since Modal reuses the same containers when possible.
#
# The way to implement this is to turn the Modal function into a method on a
# class that also implement the Python context manager interface, meaning it
# has the `__enter__` method (the `__exit__` method is optional).
#
# We have also have applied a few model optimizations to make the model run
# faster. On an A10G, the model takes about 6.5s to load into memory, and then
# 1.6s per generation on average. On a T4, it takes 13s to load and 3.7s per
# generation. Other optimizations are also available [here](https://huggingface.co/docs/diffusers/optimization/fp16#memory-and-speed).

# This is our Modal function. The function runs through the `StableDiffusionPipeline` pipeline.
# It sends the PIL image back to our CLI where we save the resulting image in a local file.


@stub.cls(gpu=gpu.A100(memory=40),timeout=700)
class WizardLM:
    def __enter__(self):
        import time
        import torch
        from transformers import  LlamaTokenizer, LlamaForCausalLM
        t0 = time.time()    

        torch.backends.cuda.matmul.allow_tf32 = True
       
        self.tokenizer =  LlamaTokenizer.from_pretrained(MODEL_ID)
        self.model = LlamaForCausalLM.from_pretrained(
            MODEL_ID,
            load_in_8bit=LOAD_8BIT,
            torch_dtype=torch.float16,
            device_map="auto",
        )


        self.model.config.pad_token_id = self.tokenizer.pad_token_id = 0  # unk
        self.model.config.bos_token_id = 1
        self.model.config.eos_token_id = 2

        if not LOAD_8BIT:
            self.model.half()

        self.model.eval()
        if torch.__version__ >= "2":
            self.model = torch.compile(self.model)
        
        total_time = time.time() - t0

        print("Init took {:.3f}s".format(total_time))

    @method()
    def run_inference(
        self,
        incoming_request: IncomingRequest,
        **kwargs,
    ) -> list[bytes]:
        import torch
        from transformers import GenerationConfig

        def generate_prompt(instruction):
            return f"""{instruction}

### Response:
"""

        prompts = generate_prompt(incoming_request.prompt)
        inputs = self.tokenizer(prompts, return_tensors="pt",
                        max_length=1024, truncation=True, padding=True)
        input_ids = inputs["input_ids"].to("cuda")

        generation_config = GenerationConfig(
            temperature=incoming_request.temperature,
            top_p=incoming_request.top_p,
            top_k=incoming_request.top_k,
            num_beams=incoming_request.num_beams,
            **kwargs,
        )
        with torch.no_grad():
            generation_output = self.model.generate(
                input_ids=input_ids,
                generation_config=generation_config,
                return_dict_in_generate=True,
                output_scores=True,
                max_new_tokens=incoming_request.max_new_tokens,
            )
        s = generation_output.sequences

        output = self.tokenizer.batch_decode(s, skip_special_tokens=True)
        
        return output[0].split("### Response:")[1].strip()


# This is the command we'll use to generate images. It takes a `prompt`,
# `samples` (the number of images you want to generate), `steps` which
# configures the number of inference steps the model will make, and `batch_size`
# which determines how many images to generate for a given prompt.

class IncomingRequest(BaseModel):
    prompt: str="Write a story about a wizard and a dragon.",
    temperature:int=1,
    top_p:float=0.9,
    top_k:int=40,
    num_beams:int=1,
    max_new_tokens:int=2048,

web_app = FastAPI()

@web_app.get("/")
async def handle_root(user_agent: Optional[str] = Header(None)):
    print(f"GET /     - received user_agent={user_agent}")
    return (f"LM {MODEL_ID}")

@web_app.post("/")
async def handle_foo(incoming_request: IncomingRequest, user_agent: Optional[str] = Header(None)):
    import time
    print(
        f"POST /foo - received user_agent={user_agent}| request={incoming_request} |"
    )

    WLM = WizardLM()
    
    t0 = time.time()    
    final_output = WLM.run_inference.call(incoming_request)
    total_time = time.time() - t0
    
    print(
        f"Prompt {incoming_request.prompt} took {total_time:.3f}s ({(total_time)/len(incoming_request.prompt):.3f}s / character). | received user_agent={user_agent} |"
    )

    return {
        "prompt": incoming_request.prompt,
        "wizardlm": final_output
    }


@stub.function(image=image,timeout=700)
@asgi_app()
def fastapi_app():
    return web_app


if __name__ == "__main__":
    stub.deploy("webapp")